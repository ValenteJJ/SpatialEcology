---
title: "Lab 3 - Multi-scale analyses"
output: html_notebook
---

# Background



Let's load the required packages. Make sure you install those that you don't already have installed if needed with install.packages().

```{r, warning=F, error=F, message=F}

setwd("C:/Users/jjv0016/OneDrive - Auburn University/Teaching/Spatial Ecology/Git/Week3")

require(FedData)
library(tidyverse)
library(terra)
library(sf)
# library(raster)      #for raster data; version 2.6-7 used
# library(rgdal)       #for raster data, projections; version 1.3-4 used
# library(rgeos)       #for buffer analysis; version 0.3-28 used
# library(fields)      #for kernel analysis; version 9.6 used
# library(Matrix)      #for kernel analysis, version 1.2-14 used
```

# Experimenting with grain and extent


To start with, we are going to simulate a simple raster. The rast() function in the terra package is generally what you're going to use to create rasters. You can make them from scratch, make them from matrices, or import them from existing raster files on your computer. Here we are just creating one from scratch by specifying we want a 6-by-6 matrix with minimum and maximum x and y coordinates set at 1 and 6.

*Play around with the rast() function at some point. There are lots of variables you can set. For example, here we are specifying the number of columns and rows, as well as a minimum and maximum coordinates for the raster's extent. Thus, the grain size of each cell is going to be 1 unit due to math. But there are dozens of other ways to specify characteristics of rasters.*
```{r}
simpRast = rast(ncol=6, nrow=6, xmin=1, xmax=6, ymin=1, ymax=6)
plot(simpRast)
```

Note that we don't get anything when we ask it to plot this raster because it's empty - we haven't put any values in the cells. So we're going to fill those cell values with random variables drawn from a Poisson distribution with a mean/variance of 3.
```{r}
set.seed(23)

simpRast[] = rpois(ncell(simpRast), lambda=3)

#plot
plot(simpRast, axes=F, box=F)
text(simpRast, digits=2)
```
Now we can see that we have a 6-by-6 matrix each containing a random positive integer. Because I set the seed, you should be seeing a matrix that is identical to mine. Note the raster starts filling in values from the top left to the bottom right. To visualize this, we can create a second raster and fill it in with the values 1-36.

```{r}
ncell(simpRast)
orderRast <- simpRast
orderRast[] <- 1:ncell(simpRast)

#plot
plot(orderRast)
text(orderRast, digits=2)
```

### Coarser grains

Ok, we built a raster and now we can alter the grain. Right now, the grain size is 1 unit in both the x and y direction. But pretend I want to double the grain size. I might do this if I simply don't need this fine of a resolution, if I am trying to match the grain size to a second raster, or if there are just too many cells and it's taking my computer too long to run. We do this with the aggregate() function in terra. One thing we have to think about is when we join multiple cells into a single cell, we need to do something to combine the values. A couple of common ways to do this are to calculate either the mean, or the mode.

```{r}

#increase the grain and calculate the mean values
simpRastMean <- aggregate(simpRast, fact=2, fun='mean')#mean value

#plot mean rule
plot(simpRastMean)
text(simpRastMean,digits=1)
```

```{r}

#increase the grain and calculate the mode
simpRastMode <- aggregate(simpRast, fact=2, fun='modal')#majority rule

#plot majority rule
plot(simpRastMode)
text(simpRastMode)
```



*Food for thought: When might you use mean vs. mode?*

Let's compare some simple statistics between the original raster and the aggregated rasters. You can calculate raster statistics using either the global() function, or by converting to a matrix and running some of the common arithmetic functions on the matrix.
```{r}

#Calculate the mean and variance of the original raster using the global function
global(simpRast, mean)
global(simpRast, var)

#Compare means
print('Means')
c(mean(as.matrix(simpRast)), mean(as.matrix(simpRastMean)), mean(as.matrix(simpRastMode)))


#Compare variances
print('Variances')
c(var(as.matrix(simpRast)), var(as.matrix(simpRastMean)), var(as.matrix(simpRastMode)))


```

*What happens to the mean and variance when you increase the grain?*


### Finer grains

Let's go the opposite direction and subset the cells to create a finer grain. We can do this with the disagg() function in the terra package. Here there are really only 2 options for setting new values to the cells. The most intuitive is to simply give it the value of the "nearest" cell that previously existed. In this case, that happens to be the cell that it used to be part of.

```{r}
#decrease the grain
simpRastNear <- disagg(simpRast, fact=2)

#plot
plot(simpRastNear, axes=F, box=F)
text(simpRastNear, cex=0.9)
```

QUICK TANGENT: For funsies, I'm going to teach you how to convert this raster to a shapefile in which all cells with the same value have been grouped into "patches." I can think of half a million uses for such a shapefile on the back end, so I thought it was worth seeing.

```{r}
tmp1 = as.polygons(simpRastNear)
tmp2 = st_as_sf(tmp1)
tmp3 = st_cast(tmp2, 'MULTIPOLYGON')
newShapefile = st_cast(tmp3, 'POLYGON')

ggplot(data=newShapefile)+
  geom_sf()+
  geom_sf_text(aes(label=lyr.1))

```

Ok, back to what we were doing. We can also use bilinear interpolation to calculate new values of cells when we disaggregate them. This means that each new cell is going to have a new value based on a distance-weighted average of values in the x and y direction.

```{r}
#decrease the grain
simpRastBil <- disagg(simpRast, fact=2, method='bilinear')

#plot
plot(simpRastBil, axes=F, box=F)
text(simpRastBil, digits=1, cex=0.6)
```

### Decrease the extent

Decreasing the extent of a raster is pretty simple as well. All we have to do is create a new extent object (essentially a shapefile) and then crop the raster.

```{r}
#decrease the extent
e <- ext(2, 4, 2, 4)#first create new, smaller extent
simpRastCrop <- crop(simpRast, e)

#plot
plot(simpRast)
plot(e, add=T)
plot(simpRastCrop)
```
Increasing the extent of the raster is similarly easy.

```{r}
#increase the extent
e <- ext(0, 7, 0, 7)#first create new, bigger extent
simpRastExt <- extend(simpRast,e)

#plot
plot(simpRast)
plot(simpRastExt)
```

Note that the new raster simply has NA values in all of the cells that we added, which could be filled in with our own values if we wanted to do that.
```{r}
matrix(simpRastExt, nrow=8)
```


# Multiscale responses to land cover

Our goal in this next exercise is going to be to identify the scale at which a species responds to a habitat characteristic. We are going to be working with a case study species, the five-lined skink and examining the effects of forest cover on its presence/absence at multiple spatial scales. This example was borrowed and tweaked from Fletcher & Fortin (2018).

Reptiles were sampled with drift-fences at 85 sites across the southeastern United States (See Figure 2 in https://onlinelibrary.wiley.com/doi/full/10.1111/gcbb.12453). Sampling occurred in mature longleaf pine savannas, and slash/loblolly pine plantations in three geographic regions in the southeastern Coastal Plains (Alabama, Georgia, and Florida). Two drift fences were used at each site and sampled for 3 days each month in April through July of 2013-2015.

To quantify forest cover, we are going to be relying on the 2011 National Land Cover Database (NLCD). NLCD is built from Landsat data and divides the entire USA into into one of 20 classification categories at a 30 m pixel resolution or grain. First let's import an NLCD raster layer that has been clipped to the Southeast.

```{r}

tmp = st_read(url('https://github.com/ValenteJJ/SpatialEcology/blob/main/Week3'), layer='reptiledata')

tmp = st_read(dsn='https://github.com/ValenteJJ/SpatialEcology/blob/main/Week3', layer='reptiledata.shp?raw=true')

# #------------------#
# #nlcd
# #------------------#
# 
# nlcd<-rast("C:/Users/jjv0016/OneDrive - Auburn University/Teaching/Spatial Ecology/Fletcher_Fortin-2018-Supporting_Files/data/nlcd2011SE")
# 
# #inspect
# proj4string(nlcd)   #from sp
# projection(nlcd)    #alternative function from raster package
# crs(nlcd)           #alternative function from raster package (replaces 'projection')
# 
# #set projection
# nlcd_proj <- projection(nlcd)
# 
# #inspect raster properties
# res(nlcd)
# ncell(nlcd)
# extent(nlcd)
# 
# #check raster values
# levels(nlcd)
# nlcd <- as.factor(nlcd) #convert to factors
# levels(nlcd)
# 
# #-------------------------------#
# #site locations: shp file
# #-------------------------------#
# 
# #site and reptile data
# sites <- readOGR("reptiledata")
# 
# #inspect
# class(sites)
# proj4string(sites)
# proj4string(sites) <- nlcd_proj #set projection
# summary(sites)
# head(sites, 2)
# 
# #plot with custom color scheme
# my_col <- c("black","blue","darkorange","red","darkred","grey30","grey50", "lightgreen",
#             "green", "darkgreen", "yellow", "goldenrod", "purple", "orchid","lightblue", "lightcyan")
# 
# #plot
# plot(nlcd, col=my_col, axes=F, box=F)
# plot(sites, add=T)
# 
# #subset points to remove corn land use
# sites <- subset(sites, management!="Corn")
# nrow(sites)
# 
# #crop raster to 10 km from sampling points: determine min/max coordinates for new extent
# x.min <- min(sites$coords_x1) - 10000
# x.max <- max(sites$coords_x1) + 10000
# y.min <- min(sites$coords_x2) - 10000
# y.max <- max(sites$coords_x2) + 10000
# 
# extent.new <- extent(x.min, x.max, y.min, y.max)
# nlcd <- crop(nlcd, extent.new)
# 
# #create a binary forest layer using nlcd as template
# forest <- nlcd
# values(forest) <- 0 #set to zero
# 
# #reclassify:
# #with raster algebra; this is slow
# forest[nlcd==41 | nlcd==42 | nlcd==43] <- 1  #locations with evergreen + mixed forest + deciduous forest
# 
# #reclassify with reclassify function is faster
# reclass <- c(rep(0,7), rep(1,3), rep(0,6))
# nlcd.levels <- levels(nlcd)[[1]]
# 
# #create reclassify matrix: first col: orginal; second: change to
# reclass.mat <- cbind(levels(nlcd)[[1]], reclass)
# reclass.mat
# 
# #reclassify
# forest <- reclassify(nlcd, reclass.mat)
# 
# #plot
# plot(forest)
# plot(sites, pch=21, col="white", add=T)
# 
# buf1km <- 1000
# buf5km <- 5000
# 
# #buffer first site
# buffer.site1.1km <- buffer(sites[1,], width=buf1km)
# plot(buffer.site1.1km)
# 
# #buffer using rgeos, which is more flexible
# buffer.site1.1km <- gBuffer(sites[1,], width=buf1km, quadsegs=10)
# buffer.site1.5km <- gBuffer(sites[1,], width=buf5km, quadsegs=10)
# 
# #zoom in on plot for 5 km buffer at site 1
# #can provide object to zoom on or click twice on layer
# 
# #plot
# zoom(nlcd, buffer.site1.5km, col=my_col, box=F)
# plot(buffer.site1.1km, border="red", lwd = 3, add=T)
# plot(buffer.site1.5km, border="red", lwd = 3, add=T)
# points(sites[1,], pch=19, cex=2)
# plot(sites[1,], col="grey20", bg="black", pch=22, cex=1, add=T)
# 
# #view just forest within buffer
# zoom(forest, buffer.site1.1km, box=F)
# plot(buffer.site1.1km, border="red", lwd = 3,add=T)
# dev.off() #end zooming
# 
# #calculate forest area within buffer
# buffer.forest1.1km <- crop(forest, buffer.site1.1km)
# buffer.forest1.1km <- mask(buffer.forest1.1km, buffer.site1.1km)
# 
# #plot forest within buffer
# plot(buffer.forest1.1km)
# 
# #calculate percent forest cover
# grainarea <- res(forest)[[1]]^2/10000#in ha
# bufferarea <- (3.14159*buf1km^2)/10000#pi*r^2
# forestcover1km <- cellStats(buffer.forest1.1km, 'sum')*grainarea
# percentforest1km <- forestcover1km/bufferarea*100
# percentforest1km
# 
# #-----------------------------------------#
# #Function that puts all the steps together
# #requires:
# #  points: one set of x,y coordinates
# #  size: the buffer size (radius), in m
# #  landcover: a binary land-cover map
# #  grain: the resolution of the map
# #-----------------------------------------#
# 
# BufferCover <- function(coords, size, landcover, grain){
# 
#   bufferarea.i <- pi*size^2/10000                             #ha; size must be in m
#   coords.i <- SpatialPoints(cbind(coords[i, 1],coords[i, 2])) #create spatial points from coordinates
#   buffer.i <- gBuffer(coords.i, width=size)                   #buffer from rgeos
#   crop.i <- crop(landcover, buffer.i)                         #crop with raster function
#   crop.NA <- setValues(crop.i, NA)                            #empty raster for the rasterization
#   buffer.r <- rasterize(buffer.i, crop.NA)                    #rasterize buffer
#   land.buffer <- mask(x=crop.i, mask=buffer.r)                #mask by putting NA outside the boundary
#   coveramount<-cellStats(land.buffer, 'sum')*grain            #calculate area
#   percentcover<-100*(coveramount/bufferarea.i)                #convert to %
# 
#   return(percentcover)
# }
# 
# #create empty vector for storing output
# f1km <- rep(NA, length = nrow(sites))
# f2km <- rep(NA, length = nrow(sites))
# 
# #with for loop (all five buffers: 910s; <=3km: 228s)
# for(i in 1:nrow(sites)) {
#   f1km[i] <- BufferCover(coords=sites,size=1000,landcover=forest,grain=grainarea)
#   f2km[i] <- BufferCover(coords=sites,size=2000,landcover=forest,grain=grainarea)
#   print(i)
# }
# 
# #make a data frame
# forest.scale <- data.frame(site=sites$site,
#                          x=sites$coords_x1, y=sites$coords_x2,
#                          f1km=f1km, f2km=f2km)
# 
# #plot
# plot(f1km, f2km)
# 
# #aggregate forest landcover to grain reflected in sampling
# forest200 <- aggregate(forest, fact=7, fun=modal)
# 
# #inspect
# res(forest200) #210x210 res
# 
# ####################################
# #2.3.4.2 Scale of effect
# ####################################
# 
# #----------------------------------------#
# #2.3.4.2 Buffer analysis
# #----------------------------------------#
# 
# #herp data
# flsk <- read.csv("reptiles_flsk.csv", header=T)
# flsk <- merge(flsk, forest.scale, by="site", all=F)
# 
# #glms at 2 scales; see text for more scales considered
# pres.1km <- glm(pres ~ f1km, family = "binomial", data = flsk)
# pres.2km <- glm(pres ~ f2km, family = "binomial", data = flsk)
# 
# #summary information
# summary(pres.1km)
# summary(pres.2km)
# 
# #likelihoods
# logLik(pres.1km)
# logLik(pres.2km)
# 
# #accessing coefficients
# pres.1km.ci <- confint(pres.1km)
# pres.2km.ci <- confint(pres.2km)
# pres.2km.ci
# 
# #----------------------------------------#
# #2.3.4.2 Kernel analysis
# #----------------------------------------#
# 
# #recreate simple logistic regression from scratch
# nll <- function(par, cov, y) {
#   alpha <- par[1]
#   beta  <- par[2]
#   lp <- alpha + beta*cov
#   p <- plogis(lp)
#   loglike <- -sum(y*log(p) + (1-y)*log(1-p))#nll
#   return(loglike)
# }
# 
# #fit simple logistic regression model with optim
# lr.buffer <- optim(par=c(0, 0), fn=nll, method='L-BFGS-B',
#                   cov=flsk$f2km, y=flsk$pres, hessian=T)
# lr.buffer$par
# lr.buffer.vc <- solve(lr.buffer$hessian)  #var-cov matrix
# lr.buffer.se <- sqrt(diag(lr.buffer.vc))  #standard errors
# lr.buffer.se
# 
# #check against glm function
# summary(pres.2km)$coefficients
# 
# #logistic regression with spatial kernel
# nll.kernel <- function(par, D, cov, y) {
#   sig <-   exp(par[1])                       #ensures sigma is non-negative
#   alpha <- par[2]
#   beta <-  par[3]
#   cov.w <- apply(D, 1, function(x) {
#     w0 <- exp(-x^2 / (2*sig^2))              #gaussian kernel
#     w0[w0==1] <- 0                           #for truncated data: D > max leads to w0=1
#     w <- w0/sum(w0)                          #w kernel weights; make so weights sum to 1
#     sum(cov * w)                             #weighted average of the raster values
#   })
#   lp <- alpha + beta*cov.w                   #linear predictor
#   p <-  plogis(lp)                           #back-transform to probability
#   loglike <- -sum(y*log(p) + (1-y)*log(1-p)) #nll for logistic
#   return(loglike)
# }
# 
# #raster to data frame
# forest200.df <- data.frame(rasterToPoints(forest200))
# 
# #inspect
# names(forest200.df)
# 
# #dist matrix
# D <- rdist(as.matrix(flsk[,c("x","y")]),
#            as.matrix(forest200.df[,c("x","y")]))
# 
# #reduce size of D
# D <- D/1000                                                         #in km
# D[D > 10] <- 0                                                      #truncate to only consider max dist (10km)
# D <- Matrix(D, sparse = TRUE)                                       #convert ot sparse matrix format
# cov.subset <- which(colSums(D)!=0, arr.ind = T)                     #identify columns with all zeros->locations that are not within 10km of any site
# D <- D[, cov.subset]                                                #remove columns with all zeros
# 
# #fit kernel logistic regression with optim:
# lr.kernel <- optim(fn=nll.kernel, hessian=T, method='L-BFGS-B',
#                    D=D, par=c(0,-6,8),                              #use starting values based on lr.buffer, adjusted for proportion rather than % cover
#                    cov=forest200.df$layer[cov.subset], y=flsk$pres)
# 
# #parameters
# lr.kernel$par
# 
# #model negative log-likelihoods
# lr.buffer$value
# lr.kernel$value
# 
# #AIC
# AICkernel <- 2*lr.kernel$value + 2*length(lr.kernel$par)
# AICbuffer <- 2*lr.buffer$value + 2*length(lr.buffer$par)
# c(AICkernel, AICbuffer)
# 
# #to plot kernel fitted in lr.kernel
# sig.fit <- exp(lr.kernel$par[1])
# dist.kernel <- seq(1,10, by=0.2)
# gaus.kernel <- exp(-dist.kernel^2/(2*sig.fit^2))
# plot(dist.kernel, gaus.kernel)
# 
# #to compare covariates:
# cov.fit.w <- apply(D, 1, function(x) {
#   w0 <- exp(-x^2 / (2*sig.fit^2))          #gaussian kernel
#   w0[w0==1] <- 0                           #for truncated data: D > max leads to w0=1
#   w <- w0/sum(w0)                          #w kernel weights; make so weights sum to 1
#   sum(forest200.df$layer[cov.subset] * w)  #weighted average of the raster values
# })
# 
# #plot
# plot(flsk$f2km/100, cov.fit.w)
# 
# #correlation
# cor(flsk$f2km/100, cov.fit.w)

```

