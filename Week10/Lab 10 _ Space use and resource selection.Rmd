---
title: "Lab 10 - Space use and resource selection"
output:
  html_document:
    df_print: paged
---

```{r}
require(tidyverse)
require(terra)
require(tidyterra)
require(sf)
require(adehabitatHR)
require(adehabitatLT)
```

# Quick background

For this lab, we're going to be using radiotelemetry data from Florida panthers (*Puma concolor coryi*) recorded in south Florida. The Florida panther is critically endangered, and there has been a push to understand space use, home ranges, and resource selection. After being collared with radio transmitters, individual panthers are relocated periodically (every 1-3 days) using airplanes. Most of the telemetry locations are recorded during the morning (7:00 am to 12:00 pm) which is a period of time when panthers are typically resting.

We're going to be using data from 6 panthers, 3 of which are adults, and 3 of which are subadults. We're going to first quantify utilization distributions for these cats, then we will contrast different approaches to understanding resource selection which vary in how they capture spatial and temporal scale.

# The data

First we are going to bring in a landcover raster. This landcover map has been aggregated at a 500 m resolution which reflects the approximate grain of telemetry error estimated in this study (489 m). This map was generated by the Florida Fish and Wildlife Commission and contains 43 landcover types, so we're going to simplify it a bit.

```{r}


land = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week10/panther_landcover.tif')


classification = read.table('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week10/landcover%20reclass.txt', header=T) 

head(classification)

unique(classification$Description2)

land = classify(land, classification[,c(1,3)])
land = categories(land, value=unique(classification[,c(3,4)]))
plot(land)


```



Now we're going to bring in the panther telemetry location data. Note that each observation has a X and Y coordinate, an ID for the individual animal, its age class, and a date on which the observation was recorded. 



```{r}

panthers = st_read('/vsicurl/https://github.com/ValenteJJ/SpatialEcology/raw/main/Week10/panthers.shp') %>% 
  mutate(CatID = as.factor(CatID))

summary(panthers)
unique(panthers$CatID)

```

Note that the CRS for the raster and shapefile is the same, so we don't have to do any spatial transformations. Therefore, we can just plot them on top of one another.

```{r}

crs(land, proj=T) == crs(panthers, proj=T)


ggplot()+
  geom_spatraster(data = land, aes(fill=Description2))+
  scale_fill_manual(values = terrain.colors(14))+
  geom_sf(data = panthers, aes(shape=CatID))

```


We're also going to create layers that represent focal neighborhood summaries of upland and wetland forests which have been shown to be important for Florida panther resource selection. We're going to calculate the proportion of each forest type within a 5 km radius which is a spatial scale that roughly reflects the median distance moved between successive points in this dataset.

```{r}

#Wet forest
wetForest = land
values(wetForest) = 0
wetForest[land %in% c(10,12)] = 1

probMatrix = focalMat(wetForest, 5000, type='circle', fillNA=FALSE)
wetFocal = focal(wetForest, probMatrix, fun='sum', na.rm=T)


#Dry forest
dryForest = land
values(dryForest) = 0
dryForest[land %in% c(11, 13)] = 1

probMatrix = focalMat(dryForest, 5000, type='circle', fillNA=FALSE)
dryFocal = focal(dryForest, probMatrix, fun='sum', na.rm=T)

```

Now we can stack these rasters up and visualize them.

```{r}

layers = c(land, wetFocal, dryFocal)
names(layers) = c('landcover', 'wetForest', 'dryForest')
plot(layers)

```

# Home range analysis

### Minimum convex polygon

Recall that the simplest way to identify the home range of an individual is to simply draw a minimum convex polygon around the points recorded on that particular panther. We can do this with the mcp function in the adehabitatHR package. Here we are drawing home range polygons based on 95% of the points and 50% of the points (the latter of which is often referred to as the "core area" for the individual). Basically, we are specifying what proportion of the most extreme values are being eliminated from consideration (5% or 50%). 

```{r}

panthersSp = as(panthers, 'Spatial')

mcp95 = mcp(panthersSp[,'CatID'], percent = 95, unin='m', unout='km2')
mcp50 = mcp(panthersSp[,'CatID'], percent = 50, unin='m', unout='km2')

mcp95Sf = st_as_sf(mcp95)
mcp50Sf = st_as_sf(mcp50)

mcp95
mcp50


```

Note that the function spits out for us a shapefile as well as summary information about the size of each of those shapefiles. We can then plot them with our study area and sample points.

```{r}

ggplot()+
  geom_spatraster(data = land, aes(fill=Description2))+
  scale_fill_manual(values = terrain.colors(14))+
  geom_sf(data = panthers, aes(shape=CatID))+
  geom_sf(data = mcp95Sf, alpha=0.5)+
  ggtitle('95% MCP')
  
ggplot()+
  geom_spatraster(data = land, aes(fill=Description2))+
  scale_fill_manual(values = terrain.colors(14))+
  geom_sf(data = panthers, aes(shape=CatID))+
  geom_sf(data = mcp50Sf, alpha=0.5)+
  ggtitle('95% MCP')
```


### Kernel density estimates

Let's get a bit more nuanced. We're now going to use the kernelUD() function to calculate kernel density estimates of home ranges for the 6 individuals. Recall that we have to specify both a bandwidth with "h" and a kernel function with "kern". Here we're stating that we want to use the Epanechnikov kernel (commonly used). In addition, rather than specifying a specific number for our bandwidth, we are using an algorithm to choose the bandwidth for us based on the variance in the x and y coordinates (href). There are other algorithms one can use (e.g., LSCV - least-square cross validation) but we're just going simple here.


```{r}
kernelHrefEpa = kernelUD(panthersSp[,'CatID'], h='href', kern='epa')

image(kernelHrefEpa)

kernelHrefEpa[[2]]@h
plot(kernelHrefEpa[[2]])

kernel.area(kernelHrefEpa)

plot(getverticeshr(kernelHrefEpa))

kde95Sf = st_as_sf(getverticeshr(kernelHrefEpa, percent=95))
kde50Sf = st_as_sf(getverticeshr(kernelHrefEpa, percent=50))

```

### Local convex hull (LoCoH)

As always, LoCoH approaches require some decisions. Here, we specifically need to decide how we identify the points that should be aggregated to draw each of the sub-polygons which we will unite to make our home range. We can use a couple of different philosophies:

* k-LoCoH: fixed number of k points considered
* r-LoCoH: fixed sphere of influence - fixed radius within which neighbors are considered
* a-LoCoH: adaptive sphere of influence - a represents the maximum sum of the distances of nearest neighbors that should be considered for each point

You can utilize all of these philosophies within the adehabitatHR package, but here we're going to focus on k-LoCoH and a-LoCoH for demonstration purposes.

In addition, regardless of which philosophy you choose, you need to find the best parameter value for defining the boundaries for neighbors to include. Usually we do this by starting at some small value and increasing it to evaluate how that changes our estimates of the home range area. As k, r, or a increases, there will be fewer "holes' leading to a home range polygon with a coarser grain. Ideally, the home range size will level off once all spurious holes are covered then increase again when one or more real holes become covered (Getz et al. 2007).

To identify the appropriate value, it is recommended you start k at N^0.5 (where N is the number of points for the individual) and increase slowly. For a, it is suggested you start at the maximum distance between pairs of points.


```{r}
panther147 = panthersSp[panthersSp$CatID==147,]

kInit = round(nrow(coordinates(panther147))^0.5, 0)
aInit = round(max(dist(coordinates(panther147))),0)

kSearch = seq(kInit, 10*kInit, by=5) #number of points
aSearch = seq(aInit, 2*aInit, by=3000) #distance in m

locohArange = LoCoH.a.area(SpatialPoints(coordinates(panther147)), arange=aSearch)
locohKrange = LoCoH.k.area(SpatialPoints(coordinates(panther147)), krange=kSearch)

```

From this, we can see that the rate of increase in home range area slows somewhere around the 5th value of a. It's more subtle with k, but there seems to be a slowing of the increase somewhere around the 11th value considered. So we'll use these values to calculate our LoCoH home ranges below.

```{r}
aSearch[5]
kSearch[11]

locohA = LoCoH.a(SpatialPoints(coordinates(panther147)), a=aSearch[5])
plot(locohA)

locohK = LoCoH.k(SpatialPoints(coordinates(panther147)), k = kSearch[11])
plot(locohK)

locohASf = st_as_sf(locohA)
locohKSf = st_as_sf(locohK)
```

Note that every single point gets a polygon. The function then starts by reporting the smallest polygon, then the two smallest merged together, then the three smallest, etc. until we have 123 total polygons. In this way, you can pull out something close to a 50% or a 95% home range by selecting the polygon with the area value closest to the percent you're interested in.

```{r}
# view(locohASf)

locohA56Sf = locohASf[26,]
locohA92Sf = locohASf[97,]

#And of course, you can do the same with your output from locohK
```


### Brownian bridge movement models

Fitting Brownian bridge home range models requires a bit of a different format for the data that captures the trajectory and temporal relationship between points. We can use the adehabitatLT package to create a trajectory object. 

```{r}

substrRight = function(x, n){
  substr(x, nchar(x) - n+1, nchar(x))
}

panthersSp = panthers %>% 
  mutate(Juldate = as.character(Juldate)) %>% 
  mutate(date = as.numeric(substrRight(Juldate, 3))) %>% 
  mutate(Date = as.Date(date, origin=as.Date("2006-01-01"))) %>% 
  mutate(Date = as.POSIXct(Date, "%Y-%m-%d")) %>% 
  as('Spatial')

pantherLtraj = as.ltraj(xy=coordinates(panthersSp), date=panthersSp$Date, id=panthersSp$CatID, typeII=T)

plot(pantherLtraj)
```

We have to estimate two parameters to fit a Brownian bridge model. The first is sig1 which relates to the speed of the individual. As sig1 increases, the assumed path tortuosity increases. We also need to estimate sig2 which is similar to h in KDE and reflects the error in location data. Increases in sig2 leads to increases in smoothing.

We estimate sig1 with maximum likelihood using the liker() function.

```{r}
sigma1 = liker(pantherLtraj, sig2=450, rangesig1=c(2, 100))

sigma1
```

Because there is substantial variation in sig1, we're going to fit the Brownian bridge model separately for each panther. We will do this for panther 147 for demonstration purposes.

```{r}
bb147 = kernelbb(pantherLtraj[6], sig=7.2, sig2=450, grid=500)
plot(bb147)

bb95Sf = st_as_sf(getverticeshr(bb147, percent=95))
bb50Sf = st_as_sf(getverticeshr(bb147, percent=50))
```



# Quantifying habitat within a home range

```{r}

habMcp50 = extract(land, mcp50Sf) %>% 
  rename(landcover = Description2) %>% 
  group_by(ID, landcover) %>% 
  summarise(habCells = n()) %>% 
  ungroup() %>% 
  group_by(ID) %>% 
  mutate(totCells = sum(habCells)) %>% 
  ungroup() %>% 
  mutate(propCells = habCells/totCells) %>% 
  pivot_wider(id_cols = ID, names_from = landcover, values_from = propCells, values_fill=0) %>% 
  mutate(ID = mcp50Sf$id) 

habMcp50



```

